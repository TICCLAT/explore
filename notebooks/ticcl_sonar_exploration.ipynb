{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what we can do with my first TICCL runs! Done on 4 SoNaR subcorpora:\n",
    "\n",
    "1. Newspapers\n",
    "2. Periodicals/magazines\n",
    "3. Websites\n",
    "4. Wikipedia\n",
    "\n",
    "Using two pipelines: with indexer and with indexerNT.\n",
    "\n",
    "In this notebook we'll focus on the indexer (non-NT) data.\n",
    "\n",
    "First step: load the data and get it in a format we can work with. Let's do that with `pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import functools  # lru_cache\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.optimize as scop\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import corner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Corpus():\n",
    "    \"\"\"\n",
    "    For a bit of order and structure, let's make this dummy class,\n",
    "    which may be extended later if necessary.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites = Corpus()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Freq\n",
    "_Computed with `FoLia-stats`, but in this case they were already provided._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.freq = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv', sep='\\t', names=['word', 'number', 'other_number', 'decimal_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the columns really? Perhaps the decimal number is the (cumulative) percentage of that word in the total corpus?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (websites.freq.number/websites.freq.number.sum()).cumsum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No, that doesn't really fit at all... The third column may be a running sum of the first one..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.freq.number.cumsum() == websites.freq.other_number"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, that seems to be close...\n",
    "\n",
    "# Clean\n",
    "\n",
    "_Computed with `TICCL-unk`_\n",
    "\n",
    "Anyway, it doesn't seem to be used by TICCL. In the clean files, we only see the first column being used as frequency/count, but with the added artifrq added to it in case of words in the lexicon:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.clean = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean', sep='\\t', names=['word', 'counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.clean\n",
    "len(websites.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So this list is a LOT longer than the original one. I guess a lot of lexicon words were added. Those should have count exactly 1000000000..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(websites.clean.counts == 100000000), len(websites.clean) - (sum(websites.clean.counts == 100000000) + len(websites.freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so 11811 \"words\" out of 72160 were filtered out by clean and 1015180 were added from the lexicon.\n",
    "\n",
    "The punct file should contain punctuation \"corrected\" words and the unk file should contain \"unknown\" words. Not sure what that meant anymore..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NO, THERE'S ALSO OVERLAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.punct = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.punct', sep='\\t', names=['word', 'correction'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(websites.punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.unk = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.unk', sep='\\t', names=['word', 'counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(websites.unk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.unk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most filtered out words are not saved anywhere, it seems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"Nino')Vieira\" in websites.clean.word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what words does punct contain then? The ones that even after trimming punctuation were not found to be clean? Even adding those we still have about 11k words unaccounted for, so indeed most cleaned-out words are gone.\n",
    "\n",
    "Anyway...\n",
    "## Nice plot time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.clean.plot(logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, that doesn't work because of artifrq. Let's correct for that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.clean[:10].apply(lambda x: (x['word'], x.counts), axis=1, broadcast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIS IS SOOOO SLOOOOW!\n",
    "# websites.clean_no_artifrq = websites.clean.apply(lambda x: (x['word'], x['counts']-100000000 if x['counts']>=100000000 else x['counts']), axis=1, broadcast=True)\n",
    "# better:\n",
    "websites.clean_no_artifrq = websites.clean[['word', 'counts']].copy()\n",
    "websites.clean_no_artifrq.loc[websites.clean_no_artifrq['counts'] >= 100000000, 'counts'] -= 100000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.clean_no_artifrq.plot(logx=True, logy=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can compare that / fit to a Zipf curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@functools.lru_cache(maxsize=10)\n",
    "def zipf_normalization(N, s):\n",
    "    return sum(1 / np.arange(1, N + 1)**s)\n",
    "\n",
    "\n",
    "def zipf_from_ranks(ranks, *, s=1):\n",
    "    return 1/ranks**s / zipf_normalization(len(ranks), s)\n",
    "\n",
    "\n",
    "def zipf(N, *, s=1):\n",
    "    ranks = np.arange(1, N + 1)\n",
    "    return zipf_from_ranks(ranks, s=s)\n",
    "\n",
    "\n",
    "def zipf_mandelbrot(N, *, q=0, s=1):\n",
    "    ranks_plus_q = np.arange(1, N + 1) + q\n",
    "    return zipf_from_ranks(ranks_plus_q, s=s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarkably good fit with no parameter tweaking at all!\n",
    "\n",
    "Actually, it makes sense to have an index starting from 1 here, to make plotting in log-log nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.clean_no_artifrq.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "websites.clean_no_artifrq.plot(logx=True, logy=True, ax=ax, legend=False)\n",
    "N = len(websites.clean_no_artifrq)\n",
    "ax.plot(np.arange(1, N + 1), websites.clean_no_artifrq.counts.sum() * zipf_mandelbrot(N),\n",
    "       label='zipf-mandelbrot')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What about the fraction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.semilogx(1 - websites.clean_no_artifrq.counts/(websites.clean_no_artifrq.counts.sum() * zipf_mandelbrot(N)))\n",
    "plt.ylim(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the KL-divergence (in **bits**, i.e. using `log2`) of the data compared to the theoretical Zipf-curve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLdiv(data, model):\n",
    "    data_masked = np.ma.masked_array(data, mask=data <= 0)\n",
    "    return -np.sum((data_masked * np.log2(model / data_masked)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLdiv(websites.clean_no_artifrq.counts/websites.clean_no_artifrq.counts.sum(), zipf_mandelbrot(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.5 bits, is that good? Should compare to the entropy of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    return -np.sum(p * np.log2(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entropy(websites.clean_no_artifrq.counts/websites.clean_no_artifrq.counts.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, only 5% information is lost in the Zipf-approximation of the data, i.e. you need about 5% more bits to encode the \"true\" distribution (the observed data, the counts) compared to an optimal encoding based on a Zipf-curve. This seems pretty good to me.\n",
    "\n",
    "In principle, you could try to fit the parameters on a minimum KLdiv. Let's try, why not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scop.minimize(lambda parameter_array: KLdiv(websites.clean_no_artifrq.counts/websites.clean_no_artifrq.counts.sum(), zipf_mandelbrot(N, q=parameter_array[0], s=parameter_array[1])),\n",
    "              x0=[0, 1], bounds=[(0, None), (0, None)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, nice, `q` indeed stays at zero, `s` is only a bit higher than 1 and the KL-divergence is only very slightly lower. So indeed, the \"default\" Zipf curve with power 1 is a very good fit already."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "websites.clean_no_artifrq.plot(logx=True, logy=True, ax=ax, legend=False)\n",
    "N = len(websites.clean_no_artifrq)\n",
    "ax.plot(np.arange(1, N + 1), websites.clean_no_artifrq.counts.sum() * zipf_mandelbrot(N, q=0, s=1.08957528),\n",
    "       label='zipf-mandelbrot')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, that looks slightly better by eye, but nothing amazing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back to data loading\n",
    "\n",
    "Still have a few things to load: anahash, confuslist.index, short.ldcalc, ldcalc.ambi, ldcalc and ldcalc.ranked for the non-NT run and also corpusfoci (part of anahash) for the NT run.\n",
    "\n",
    "## Anahash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash', sep='~',\n",
    "                               index_col=0, names=['anahash', 'words'])\n",
    "websites.anahash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, how do we load such a data file into Pandas efficiently? Asked data SIG. In the meantime, let's try this (https://stackoverflow.com/questions/17116814/pandas-how-do-i-split-text-in-a-column-into-multiple-rows):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anahash_first_try():\n",
    "    anahash = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash', sep='~',\n",
    "                                   index_col=0, names=['anahash', 'words'],\n",
    "                                   converters={'words': lambda w: tuple(w.split('#'))})\n",
    "    anahash['words'][:20].apply(pd.Series, 1).stack()\n",
    "    anahash['words'][:20]\n",
    "    return anahash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash = load_anahash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Really weird, some `\\n`s aren't read as newlines! Sublime Text has no problem with them... What's up? Wrong encoding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash',\n",
    "#                                sep='~', quoting=csv.QUOTE_NONE,\n",
    "#                                index_col=0, names=['anahash', 'words'],\n",
    "#                                converters={'words': lambda w: tuple(w.split('#'))}, encoding='utf-8')\n",
    "# websites.anahash.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better. Try again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash[:20]['words'].apply(pd.Series, 1).stack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome. Now in one go?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash',\n",
    "#                                sep='~', quoting=csv.QUOTE_NONE,\n",
    "#                                index_col=0, names=['anahash', 'words'],\n",
    "#                                converters={'words': lambda w: pd.Series(w.split('#'))}, encoding='utf-8')\n",
    "# websites.anahash.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok that doesn't work. Let's stick to two (or actually four) steps then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_anahash():\n",
    "    anahash_tuples_df = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash',\n",
    "                                    sep='~', quoting=csv.QUOTE_NONE,\n",
    "                                    index_col=0, names=['anahash', 'words'],\n",
    "                                    converters={'words': lambda w: tuple(w.split('#'))}, encoding='utf-8')\n",
    "    anahash = anahash_tuples_df['words'].apply(pd.Series, 1).stack().to_frame()\n",
    "    anahash.index.rename([\"anahash\", \"variant_id\"], inplace=True)\n",
    "    anahash.rename({0: 'word'}, axis='columns', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash = load_anahash()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time load_anahash()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes some time to load, so let's save the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# websites.anahash.to_msgpack('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash.msgpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash = pd.read_msgpack('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.anahash.msgpack')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great. Let's see what's in there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(websites.anahash)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd, that's two more words than the original clean list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(websites.clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, must be some empty line or some wrong handling of a comma or newline... anyway..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash.groupby('anahash').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash.groupby('anahash').count().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yeah, ok, some anagrams have more variants than others, nothing surprising. Let's see some more interesting statistics, like word length vs variants.\n",
    "\n",
    "ACCORDING TO MARTIN IT SHOULD BE ABOUT 1.3 ON AVERAGE.\n",
    "\n",
    "Assuming that all anagrams have an equal number of characters, we can just use the 0 variant_ids to count the string lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= websites.anahash\\\n",
    "      .groupby('anahash').count()\\\n",
    "      .rename({'word': 'variant_count'}, axis='columns')\\\n",
    "      .join(websites.anahash\n",
    "            .loc[(slice(None), 0), :]['word']\n",
    "            .str.len()\n",
    "            .reset_index(level='variant_id', drop=True)\n",
    "            .rename('word_length'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corner.corner(df);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also not surprising: longer words, less variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.anahash.groupby('variant_id').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# confuslist.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file is huge, 1.8G, so we need some other sort of handling, pandas will surely crash on anything but a supercomputer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.confuslist_index = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index',\n",
    "                                        memory_map=True, sep='#', nrows=10, index_col=0,\n",
    "                                        names=['confusion', 'word_hashes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "websites.confuslist_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we were to load this in similarly to the anahashes, with a MultiIndex, how much memory would that take? It would increase the number of columns to three (even though the second index could be a small int probably), so $3 * 8 = 24$ bytes per word hash. Number of word hashes is number of commas plus number of new lines in the file. This counts commas:\n",
    "```sh\n",
    "tr -cd ',' < WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index | wc -c\n",
    "```\n",
    "This counts newlines:\n",
    "```sh\n",
    "wc -l WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas = 148610673\n",
    "newlines = 250747\n",
    "commas + newlines, (commas + newlines) * 24, (commas + newlines) * 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oi, 3.5G, more than expected from just the text file size... Actually, we can probably just use uint16 for the second index and uint32 for the confusion, only the actual hashes must be uint64, so that would sum to just 14 bytes per line, a total of about 2G then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "commas/newlines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should be safely less than 65535, so then indeed a uint16 for the second column would be possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's try it out then, developed this in about 1.5 hours:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticcl_output_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit confusion_array, confusion_word_index_array, word_anahash_array = ticcl_output_reader.load_confuslist_index(\"sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confuslist_index_head():\n",
    "    df_tuples = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head',\n",
    "                        sep='#', index_col=0, names=['confusion', 'word_hashes'],\n",
    "                        converters={'word_hashes': lambda w: tuple(w.split(','))})\n",
    "    df = df_tuples['word_hashes'].apply(pd.Series, 1).stack().to_frame()\n",
    "    df.index.rename([\"confusion\", \"list_index\"], inplace=True)\n",
    "    df.rename({0: 'word_hash'}, axis='columns', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_confuslist_index_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "200/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! Is it correct though?\n",
    "\n",
    "Not immediately, had to fix some bugs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index = ticcl_output_reader.load_confuslist_index(\"sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index_df = pd.DataFrame.from_records({\"confusion\": cpp_index[0],\n",
    "                                          \"list_index\": cpp_index[1],\n",
    "                                          \"word_hash\": cpp_index[2]}, index=[\"confusion\", \"list_index\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index = get_confuslist_index_head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all(cpp_index_df.index == pandas_index.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index_df.values == pandas_index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Odd, they seem equal..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpp_index_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhh, yeah ok. That may also explain the slowness..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confuslist_index_head2():\n",
    "    df_tuples = pd.read_csv('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head',\n",
    "                        sep='#', index_col=0, names=['confusion', 'word_hashes'],\n",
    "                        converters={'word_hashes': lambda w: tuple(w.split(','))})\n",
    "    df = df_tuples['word_hashes'].apply(pd.Series, 1).stack().astype('uint64').to_frame()\n",
    "    df.index.rename([\"confusion\", \"list_index\"], inplace=True)\n",
    "    df.rename({0: 'word_hash'}, axis='columns', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index = get_confuslist_index_head2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index.equals(cpp_index_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoohoo!\n",
    "\n",
    "And timing on this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_confuslist_index_head2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Same, good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A Redditor came up with the suggestion to do it in pure Python, let's try that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confuslist_index_head_pure_python():\n",
    "    with open('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head') as f:\n",
    "        dc = {}\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            key, _, value = line.partition(\"#\")\n",
    "            values = value.rstrip(\"\\n\").split(\",\")\n",
    "            dc[int(key)] = values\n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit get_confuslist_index_head_pure_python()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Holy crap, but what does it look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_py = get_confuslist_index_head_pure_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure_py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right, so I'll need to still convert to three columns here as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_confuslist_index_head_pure_python():\n",
    "    with open('sonar_ticcl/WR-P-E-I_web_sites.wordfreqlist.tsv.clean.confuslist.index.head') as f:\n",
    "        dc = {}\n",
    "        for line in f:\n",
    "            if not line:\n",
    "                continue\n",
    "            key, _, value = line.partition(\"#\")\n",
    "            values = value.rstrip(\"\\n\").split(\",\")\n",
    "            dc[int(key)] = values\n",
    "    df = pd.DataFrame.from_dict(dc, orient='index').stack().astype('uint64').to_frame()\n",
    "    df.index.rename([\"confusion\", \"list_index\"], inplace=True)\n",
    "    df.rename({0: 'word_hash'}, axis='columns', inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pure_py_df = df_confuslist_index_head_pure_python()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_index.equals(pure_py_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, then we time that..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit df_confuslist_index_head_pure_python()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# short.ldcalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ldcalc.ambi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ldcalc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ldcalc.ranked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
