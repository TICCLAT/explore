{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s [%(process)d] %(levelname)-8s \"\n",
    "                    \"%(name)s,%(lineno)s\\t%(message)s\")\n",
    "logging.getLogger().setLevel('DEBUG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('../ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "# db_name = 'ticclat_test'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus, WordformLink, WordformLinkSource, lexical_source_wordform\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope\n",
    "\n",
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "in_dir = '/Users/jvdzwaan/data/ticclat/SGD_ticcl_variants/'\n",
    "\n",
    "in_files = glob.glob('{}/*'.format(in_dir))\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for in_file in in_files:\n",
    "    df = pd.read_csv(in_file, sep='#', header=None)\n",
    "    df.columns = ['ocr_variant', 'corpus_frequency', 'correction_candidate', '?1', 'ld', '?2', 'anahash']\n",
    "    dfs.append(df)\n",
    "data = pd.concat(dfs)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.dbutils import bulk_add_wordforms\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    # add wordforms we don't know yet (don't forget to include the correction candidates)\n",
    "    wfs = pd.DataFrame()\n",
    "    wfs['wordform'] = list(set(list(data['ocr_variant']) + list(data['correction_candidate'])))\n",
    "    print(wfs.head())\n",
    "    bulk_add_wordforms(session, wfs, preprocess_wfs=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.dbutils import update_anahashes\n",
    "\n",
    "alphabet_file=\"/Users/jvdzwaan/data/ticclat/ALPH/nld.aspell.dict.clip20.lc.LD3.charconfus.clip20.lc.chars\"\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    # make sure we have anahashes for all wordforms\n",
    "    update_anahashes(session, alphabet_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add linked lexicon (ignore anahash differences for now)\n",
    "\n",
    "from ticclat.dbutils import add_lexicon_with_links\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    name = 'SDG ticcl correction candidates'\n",
    "    vocabulary = False\n",
    "    from_column = 'ocr_variant'\n",
    "    from_correct = False\n",
    "    to_column = 'correction_candidate'\n",
    "    to_correct = True\n",
    "    preprocess_wfs = False\n",
    "    to_add = ['ld']\n",
    "\n",
    "    add_lexicon_with_links(session, name, vocabulary, data, from_column, to_column, from_correct, to_correct, preprocess_wfs=preprocess_wfs, to_add=to_add)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
