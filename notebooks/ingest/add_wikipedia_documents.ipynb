{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s [%(process)d] %(levelname)-8s \"\n",
    "                    \"%(name)s,%(lineno)s\\t%(message)s\")\n",
    "logging.getLogger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('../ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat_wikipedia'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope\n",
    "\n",
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    print('number of wordforms:', session.query(Wordform).count())\n",
    "    print('number of lexica:', session.query(Lexicon).count())\n",
    "    print('number of documents:', session.query(Document).count())\n",
    "    print('number of corpora:', session.query(Corpus).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Ingest wikipedia dump as corpus\n",
    "import os\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from ticclat.utils import get_temp_file, write_json_lines, read_json_lines\n",
    "from ticclat.tokenize import nltk_tokenize, terms_documents_matrix_word_lists\n",
    "from ticclat.sacoreutils import add_corpus_core\n",
    "\n",
    "\n",
    "wiki = '/home/jvdzwaan/data/tmp/nlwiki'\n",
    "corpus_name = 'nlwiki-20190201-pages-articles-complete'\n",
    "\n",
    "print('Tokenizing corpus')\n",
    "tokenized_file = '/home/jvdzwaan/data/tmp/nlwiki-json_lines'\n",
    "num_documents = write_json_lines(tokenized_file, tqdm(nltk_tokenize(wiki)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.tokenize import terms_documents_matrix_word_lists\n",
    "\n",
    "print('Creating the terms/document matrix')\n",
    "documents_iterator = read_json_lines(tokenized_file)\n",
    "\n",
    "corpus_m, v = terms_documents_matrix_word_lists(documents_iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove(tokenized_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "wfs = pd.DataFrame()\n",
    "wfs['wordform'] = v.vocabulary_\n",
    "\n",
    "document_metadata = pd.DataFrame()\n",
    "document_metadata['language'] = ['nl' for i in range(num_documents)]\n",
    "document_metadata['pub_year'] = 2019\n",
    "# More metadata?\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    add_corpus_core(session, corpus_m, v, corpus_name, document_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
