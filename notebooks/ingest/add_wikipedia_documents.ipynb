{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%load_ext autoreload\n%autoreload 2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import logging\n\nlogging.basicConfig(format=\"%(asctime)s [%(process)d] %(levelname)-8s \"\n                    \"%(name)s,%(lineno)s\\t%(message)s\")\nlogging.getLogger().setLevel('INFO')"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n\nfrom tqdm import tqdm_notebook as tqdm"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Read information to connect to the database and put it in environment variables\nimport os\nwith open('../ENVVARS.txt') as f:\n    for line in f:\n        parts = line.split('=')\n        if len(parts) == 2:\n            os.environ[parts[0]] = parts[1].strip()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "db_name = 'ticclat_wikipedia'\nos.environ['dbname'] = db_name"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus\n\nfrom ticclat.dbutils import get_session, session_scope\n\nSession = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "with session_scope(Session) as session:\n    print('number of wordforms:', session.query(Wordform).count())\n    print('number of lexica:', session.query(Lexicon).count())\n    print('number of documents:', session.query(Document).count())\n    print('number of corpora:', session.query(Corpus).count())"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# note: must install nltk for this! This used to be in ticclat.tokenize, but it was no longer used anywhere but in this notebook, so we took it out of the package dependencies. Note that it is also still used in some tests, but we have a separate utility function in the tests directory for that.\n\nimport nltk.data\nfrom nltk import word_tokenize\n\ndef nltk_tokenize(texts_file, punkt='tokenizers/punkt/dutch.pickle'):\n    \"\"\"\n    Inputs:\n        texts_file (str): File name of a file that contains the texts. This\n            should contain one document per line.\n        punkt (str): Path to the nltk punctuation data to be used.\n\n    Yields:\n        Counter: term-frequency vector representing a document.\n    \"\"\"\n    nltk.download('punkt')\n    tokenizer = nltk.data.load(punkt)\n\n    with open(texts_file) as f:\n        for line in f:\n            tokens = [word_tokenize(sent)\n                      for sent in tokenizer.tokenize(line.strip())]\n\n            yield list(chain(*tokens))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\n# Ingest wikipedia dump as corpus\nimport os\n\nfrom tqdm import tqdm_notebook as tqdm\n\nfrom ticclat.utils import get_temp_file, write_json_lines, read_json_lines\nfrom ticclat.tokenize import terms_documents_matrix_word_lists\nfrom ticclat.sacoreutils import add_corpus_core\n\n\nwiki = '/home/jvdzwaan/data/tmp/nlwiki'\ncorpus_name = 'nlwiki-20190201-pages-articles-complete'\n\nprint('Tokenizing corpus')\ntokenized_file = '/home/jvdzwaan/data/tmp/nlwiki-json_lines'\nnum_documents = write_json_lines(tokenized_file, tqdm(nltk_tokenize(wiki)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nfrom ticclat.tokenize import terms_documents_matrix_word_lists\n\nprint('Creating the terms/document matrix')\ndocuments_iterator = read_json_lines(tokenized_file)\n\ncorpus_m, v = terms_documents_matrix_word_lists(documents_iterator)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "os.remove(tokenized_file)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "%%time\nwfs = pd.DataFrame()\nwfs['wordform'] = v.vocabulary_\n\ndocument_metadata = pd.DataFrame()\ndocument_metadata['language'] = ['nl' for i in range(num_documents)]\ndocument_metadata['pub_year'] = 2019\n# More metadata?\n\nwith session_scope(Session) as session:\n    add_corpus_core(session, corpus_m, v, corpus_name, document_metadata)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 }
}
