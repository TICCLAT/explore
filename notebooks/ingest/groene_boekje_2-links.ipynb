{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groene boekje part 2: wordform links\n",
    "\n",
    "In [the first Groene Boekje notebook](groene_boekje.ipynb) we cleaned just for wordforms. Now we will extend that with wordform links based on the relations in the rows. For this we will probably also need to modify dbutils."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticclat.dbutils\n",
    "import ticclat.ticclat_schema\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy_utils import database_exists, create_database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# engine = create_engine(\"mysql://{}:{}@localhost/{}\".format(os.environ['user'], \n",
    "#                                                            os.environ['password'], \n",
    "#                                                            os.environ['dbname']))\n",
    "# if not database_exists(engine.url):\n",
    "#     create_database(engine.url)\n",
    "\n",
    "# print(database_exists(engine.url))\n",
    "\n",
    "# Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sqlalchemy import inspect\n",
    "\n",
    "# inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get table information\n",
    "# print(inspector.get_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Groene Boekje data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_basepath = \"/Users/pbos/projects/ticclat/data/GB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB1914_path = GB_basepath + \"1914/22722-8.txt\"\n",
    "GB1995_path = GB_basepath + \"1995-2005/1995/GB95_002.csv\"\n",
    "GB2005_path = GB_basepath + \"1995-2005/2005/GB05_002.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_GB1995 = pd.read_csv(GB1995_path, sep=';', names=[\"word\", \"syllables\", \"see also\", \"disambiguation\",\n",
    "#                                                      \"grammatical tag\", \"article\",\n",
    "#                                                      \"plural/past/attrib\", \"plural/past/attrib syllables\",\n",
    "#                                                      \"diminu/compara/past plural\", \"diminu/compara/past plural syllables\",\n",
    "#                                                      \"past perfect/superla\", \"past perfect/superla syllables\"],\n",
    "#                         encoding='utf8') # encoding necessary for later loading into sqlalchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df_GB1995[~df_GB1995[\"see also\"].isnull()].sample(10)\n",
    "# # df_GB1995[~df_GB1995[\"disambiguation\"].isnull()].sample(10)\n",
    "# df_GB1995[~df_GB1995[\"disambiguation\"].isnull() & df_GB1995[\"disambiguation\"].str.contains(' ')].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "Clean up will be different now that we also need links.\n",
    "- At first we need to retain the columns while cleaning them, because we'll need the rows to define links.\n",
    "- The cleaning of the wordform columns will be the same as before, but now per column instead of all in one go.\n",
    "- However, because we must retain rows, things like splitting on commas will now mean the entire row must be duplicated.\n",
    "\n",
    "Some notes picked up from random sample checking:\n",
    "- The disambiguation words can also have a duplicate word numbers! These are irrelevant to us, since we only deal with word forms, not semantics, so we have to remove them there as well.\n",
    "\n",
    "### Links\n",
    "The different types of links we could extract from this table are:\n",
    "\n",
    "- \"see also\" (column 3): this is usually an **spelling variant**, so very relevant in our case. Both are **correct** words.\n",
    "- \"disambiguation (column 4):\n",
    "    + This column is always between parentheses.\n",
    "    + Usually it is a semantically very similar word.\n",
    "    + Sometimes the disambiguation is in multiple words. In this case (as opposed to the multiple words in wordform columns) multiple words are more often really separate words, like `slechte waar` (for `kamelot2`) or `het slopen` (for `sloop1`), which are really two separate wordforms, not a \"multi-word wordform\". We probably can't count on this, but we could do a lookup of the separate words. Even if we did that, this column entry is really almost more like a sentence, an explanation of the word, so the separate words cannot be entered into the database as wordforms, nor can they be entered separately, because their combination makes up their meaning, which it will lose when taken apart.\n",
    "    + There are even entries that have multiple disambiguating words, separated by a comma. These can easily be used separately.\n",
    "    + A lot of them have `(andere bett.)` or (more rarely) `(andere bet.)` which I guess means \"other meaning(s)\", not sure though.\n",
    "    + So to sum up, what we can do with this column:\n",
    "        * Remove `(andere bett.)`\n",
    "        * Strip parentheses\n",
    "        * Split by comma\n",
    "        * **First approximation**: remove multi-word entries\n",
    "        * *Use remaining words as **semantic** links*\n",
    "        * Again, both are **correct** words.\n",
    "- Columns 7, 9 and 11 give us **morphological links** of different types. Again, all **correct** words.\n",
    "    + We could in principle deduce the type from the grammatical tag of the word in column 5, but will no do so for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean up links\n",
    "\n",
    "This time we clean up the dataframe as a table, not as a single row of wordforms.\n",
    "\n",
    "We can drop a few columns though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data = df_GB1995.drop([\"syllables\", \"grammatical tag\", \"article\",\n",
    "#                             \"plural/past/attrib syllables\", \"diminu/compara/past plural syllables\", \"past perfect/superla syllables\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean columns 2 and 3\n",
    "\n",
    "We have to remove `zie ook ` (see also ) from column 2.\n",
    "\n",
    "For column 3 we will:\n",
    "- strip the parentheses\n",
    "- remove multi-word entries (including especially `andere bett.`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean \"see also\" (column 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data['see also'] = link_data['see also'].str.replace('zie ook ', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data[~link_data['see also'].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean \"disambiguation\" (column 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data['disambiguation'] = link_data['disambiguation'].str.strip('()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data['disambiguation'][link_data['disambiguation'] == 'andere bett.'] = None\n",
    "# link_data['disambiguation'][link_data['disambiguation'] == 'andere bet.'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_data[~link_data['disambiguation'].isnull()].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First remove the isolated multi-words, i.e. the ones without commas.\n",
    "2. Then, if necessary (turns out, it's only 20-30 rows left), let's make a nice regex to replace multi-words, both isolated ones and ones in comma separated lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\" \", na=False)\n",
    "                            & ~link_data['disambiguation'].str.contains(\",\", na=False)] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\"[^,] \", na=False)] = (\n",
    "    link_data['disambiguation']\n",
    "     [link_data['disambiguation'].str.contains(\"[^,] \", na=False)]\n",
    "     .str.split(', ')\n",
    "     .map(lambda x: [i for i in x if not ' ' in i])\n",
    "     .map(lambda x: None if len(x) == 0 else ', '.join(x))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\"[^,] \", na=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\" \", na=False)].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One remaining pesky thing: `kippenloop, -korf`. Is this a pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\", -\", na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, so let's just get rid of it here and now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\", -\", na=False)] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty lines\n",
    "\n",
    "For links, we don't need the rows that only have a `word` entry, since these by definition have no links in that row. They may be linked to another word through the `see also` column, but then the word will also be in the row of the word where it is in the `see also` column, so the word's row itself can safely be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data = link_data.dropna(how='all', subset=[\"see also\", \"disambiguation\", \"plural/past/attrib\", \"diminu/compara/past plural\", \"past perfect/superla\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to links!\n",
    "\n",
    "Actually, for now, we can keep this rather simple: we just make a table with two columns, where the first column has the wordforms in the `word` column and the second has the wordforms in the other columns.\n",
    "\n",
    "We will then split comma separated words in a second pass to keep things simple."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = (link_data.set_index('word').stack().reset_index().drop('level_1', axis=1)\n",
    "                    .rename({'word': 'wordform_1', 0: 'wordform_2'}, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_comma1 = link_df['wordform_1'].str.contains(',')\n",
    "link_df = pd.concat((link_df[~has_comma1],) + tuple(pd.DataFrame({'wordform_1': row['wordform_1'].split(', '),\n",
    "                                                                  'wordform_2': (row['wordform_2'],) * len(row['wordform_1'].split(', '))})\n",
    "                                                     for ix, row in link_df[has_comma1].iterrows()))\n",
    "\n",
    "has_comma2 = link_df['wordform_2'].str.contains(',')\n",
    "link_df = pd.concat((link_df[~has_comma2],) + tuple(pd.DataFrame({'wordform_1': (row['wordform_1'],) * len(row['wordform_2'].split(', ')),\n",
    "                                                                  'wordform_2': row['wordform_2'].split(', ')})\n",
    "                                                     for ix, row in link_df[has_comma2].iterrows()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df.tail(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean wordforms\n",
    "\n",
    "Almost like in [the first Groene Boekje notebook](groene_boekje.ipynb), but now per column. The main changes are:\n",
    "- We do it per column\n",
    "- We must not do `.unique()` at the end, because duplicate words in a column may be linked to different words in the other column! Unique should be row-based.\n",
    "\n",
    "We also redo the above in one go in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_data = df_GB1995.drop([\"syllables\", \"grammatical tag\", \"article\",\n",
    "                            \"plural/past/attrib syllables\", \"diminu/compara/past plural syllables\", \"past perfect/superla syllables\"], axis=1)\n",
    "\n",
    "# clean link_data\n",
    "link_data['see also'] = link_data['see also'].str.replace('zie ook ', '')\n",
    "link_data['disambiguation'] = link_data['disambiguation'].str.strip('()')\n",
    "link_data['disambiguation'][link_data['disambiguation'] == 'andere bett.'] = None\n",
    "link_data['disambiguation'][link_data['disambiguation'] == 'andere bet.'] = None\n",
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\" \", na=False)\n",
    "                            & ~link_data['disambiguation'].str.contains(\",\", na=False)] = None\n",
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\"[^,] \", na=False)] = (\n",
    "    link_data['disambiguation']\n",
    "     [link_data['disambiguation'].str.contains(\"[^,] \", na=False)]\n",
    "     .str.split(', ')\n",
    "     .map(lambda x: [i for i in x if not ' ' in i])\n",
    "     .map(lambda x: None if len(x) == 0 else ', '.join(x))\n",
    ")\n",
    "link_data['disambiguation'][link_data['disambiguation'].str.contains(\", -\", na=False)] = None\n",
    "link_data = link_data.dropna(how='all', subset=[\"see also\", \"disambiguation\", \"plural/past/attrib\", \"diminu/compara/past plural\", \"past perfect/superla\"])\n",
    "\n",
    "# convert to link_df\n",
    "link_df = (link_data.set_index('word').stack().reset_index().drop('level_1', axis=1)\n",
    "                    .rename({'word': 'wordform_1', 0: 'wordform_2'}, axis=1))\n",
    "has_comma1 = link_df['wordform_1'].str.contains(',')\n",
    "link_df = pd.concat((link_df[~has_comma1],) + tuple(pd.DataFrame({'wordform_1': row['wordform_1'].split(', '),\n",
    "                                                                  'wordform_2': (row['wordform_2'],) * len(row['wordform_1'].split(', '))})\n",
    "                                                     for ix, row in link_df[has_comma1].iterrows()))\n",
    "\n",
    "has_comma2 = link_df['wordform_2'].str.contains(',')\n",
    "link_df = pd.concat((link_df[~has_comma2],) + tuple(pd.DataFrame({'wordform_1': (row['wordform_1'],) * len(row['wordform_2'].split(', ')),\n",
    "                                                                  'wordform_2': row['wordform_2'].split(', ')})\n",
    "                                                     for ix, row in link_df[has_comma2].iterrows()))\n",
    "\n",
    "link_df = link_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wordform_series(wordform_series, remove_duplicates=False):\n",
    "    # remove colons\n",
    "    wordform_series = wordform_series.str.replace(':', '')\n",
    "    # strip whitespace\n",
    "    wordform_series = wordform_series.str.strip()\n",
    "    # remove duplicate word footnote numbers\n",
    "    duplicates = wordform_series.str.contains('[0-9]$', regex=True)\n",
    "    wordform_series = pd.concat((wordform_series[~duplicates], wordform_series[duplicates].str.replace('[0-9]$', '', regex=True)))\n",
    "    # remove parentheses around some words\n",
    "    wordform_series = wordform_series.sort_values().str.strip(\"()\")\n",
    "    # remove abbreviations\n",
    "    abbreviation = wordform_series.str.contains('\\.$')\n",
    "    wordform_series = wordform_series[~abbreviation]\n",
    "    \n",
    "    if remove_duplicates:\n",
    "        wordform_series = pd.Series(wordform_series.unique())\n",
    "    return wordform_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_clean_df = link_df.copy()\n",
    "link_clean_df['wordform_1'] = clean_wordform_series(link_clean_df['wordform_1'])\n",
    "link_clean_df['wordform_2'] = clean_wordform_series(link_clean_df['wordform_2'])\n",
    "# some links will be removed (abbreviations), so drop those rows\n",
    "link_clean_df = link_clean_df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(link_df) - len(link_clean_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cleanliness_wordform_series(wordform_series, head=5):\n",
    "    print(\"Random sample:\")\n",
    "    display(wordform_series.sample(10))\n",
    "    print(\"Colons, periods:\")\n",
    "    display(wordform_series[wordform_series.str.contains(':')].head(head))\n",
    "    display(wordform_series[wordform_series.str.contains('.', regex=False)].head(head))\n",
    "    print(\"White space padding:\")\n",
    "    display(wordform_series[wordform_series.str.contains('^ | $')].head(head))\n",
    "    print(\"Trailing numbers:\")\n",
    "    display(wordform_series[wordform_series.str.contains('[0-9]$', regex=True)].head(head))\n",
    "    print(\"Parentheses:\")\n",
    "    display(wordform_series[wordform_series.str.contains('\\(|\\)', regex=True)].head(head))\n",
    "    print(\"Abbreviations:\")\n",
    "    display(wordform_series[wordform_series.str.contains('\\.$')].head(head))\n",
    "    \n",
    "    print(\"Finally, just the first entries of sorted df:\")\n",
    "    display(wordform_series.sort_values().head(head))\n",
    "    display(wordform_series.sort_values().tail(head))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cleanliness_wordform_series(link_df['wordform_1'])\n",
    "check_cleanliness_wordform_series(link_df['wordform_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cleanliness_wordform_series(link_clean_df['wordform_1'])\n",
    "check_cleanliness_wordform_series(link_clean_df['wordform_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing diacritics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that these are regex formatted, i.e. with special characters escaped\n",
    "diacritic_markers = {'@`': '\\u0300',    # accent grave\n",
    "                     \"@\\\\'\": '\\u0301',  # accent aigu\n",
    "                     '@\\\\\\\\': '\\u0308', # trema\n",
    "                     '@\\+': '\\u0327',   # cedilla\n",
    "                     '@\\^': '\\u0302',   # accent circumflex\n",
    "                     '@=': '\\u0303',    # tilde\n",
    "                     '@@': \"'\",         # apostrophe (not actually a diacritic)\n",
    "                     '@2': '\\u2082',    # subscript 2\n",
    "                     '@n': '\\u0308n'    # trema followed by n\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in link_clean_df:\n",
    "    for marker, umarker in diacritic_markers.items():\n",
    "        link_clean_df[column] = link_clean_df[column].str.replace(marker, umarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_clean_df.sort_values(by=link_clean_df.columns.tolist()).sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Groene Boekje LINKS DataFrames into TICCLAT database\n",
    "\n",
    "Here we use the example of the [Twente spelling correction notebook](twente_spelling_correction.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "\n",
    "# with ticclat.dbutils.session_scope(Session) as session:\n",
    "#     lexicon = session.query(ticclat.ticclat_schema.Lexicon).filter(ticclat.ticclat_schema.Lexicon.lexicon_name=='Groene Boekje 1995').first()\n",
    "#     if lexicon is None:\n",
    "#         raise Exception(\"No lexicon found!\")\n",
    "#     for idx, row in tqdm.tqdm(link_clean_df.iterrows(), total=link_clean_df.shape[0]):\n",
    "#         wf = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_1'].encode('utf8')).first()\n",
    "#         corr = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_2'].encode('utf8')).first()        \n",
    "#         wf.link_with_metadata(corr, True, True, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm, apparently there is a None / NaN somewhere in the table..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(link_clean_df.isna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "any(link_clean_df.index.isna()), any(link_clean_df['wordform_1'].isna()), any(link_clean_df['wordform_2'].isna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But where?\n",
    "\n",
    "Maybe the problem is with the original ingestion, there could be a wordform in this links DataFrame that was filtered out of the original wordforms DataFrame..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_clean_df.iloc[4559-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hm, yeah that quote might be problematic...\n",
    "\n",
    "## After fixing reading in of the csv file\n",
    "\n",
    "Again, all in one, with fixed original df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticclat.ingest.groene_boekje"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_df = ticclat.ingest.groene_boekje.create_GB95_link_df(ticclat.ingest.groene_boekje.load_GB95(GB1995_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %time\n",
    "\n",
    "# with ticclat.dbutils.session_scope(Session) as session:\n",
    "#     lexicon = session.query(ticclat.ticclat_schema.Lexicon).filter(ticclat.ticclat_schema.Lexicon.lexicon_name=='Groene Boekje 1995').first()\n",
    "#     if lexicon is None:\n",
    "#         raise Exception(\"No lexicon found!\")\n",
    "#     for idx, row in tqdm.tqdm(link_df.iterrows(), total=link_df.shape[0]):\n",
    "#         wf = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_1'].encode('utf8')).first()\n",
    "#         corr = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_2'].encode('utf8')).first()        \n",
    "#         wf.link_with_metadata(corr, True, True, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fuck, still the same crap it seems..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# link_df.loc[4556]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, the problem turned out to be the `engine` URL, it was missing the `?charset=utf8mb4` part! Now we can also leave off the `.encode('utf8')` crap.\n",
    "\n",
    "Ok, again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine8 = create_engine(\"mysql://{}:{}@localhost/{}?charset=utf8mb4\".format(os.environ['user'], \n",
    "                                                                            os.environ['password'], \n",
    "                                                                            os.environ['dbname']))\n",
    "Session8 = sessionmaker(bind=engine8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # %time\n",
    "\n",
    "# with ticclat.dbutils.session_scope(Session8) as session:\n",
    "#     lexicon = session.query(ticclat.ticclat_schema.Lexicon).filter(ticclat.ticclat_schema.Lexicon.lexicon_name=='Groene Boekje 1995').first()\n",
    "#     if lexicon is None:\n",
    "#         raise Exception(\"No lexicon found!\")\n",
    "#     for idx, row in tqdm.tqdm(link_df.iterrows(), total=link_df.shape[0]):\n",
    "#         wf = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_1']).first()\n",
    "#         corr = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_2']).first()\n",
    "#         if corr is None:\n",
    "#             print(row['wordform_2'])\n",
    "#         wf.link_with_metadata(corr, True, True, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... man, what now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ticclat.dbutils.session_scope(Session8) as session:\n",
    "    lexicon = session.query(ticclat.ticclat_schema.Lexicon).filter(ticclat.ticclat_schema.Lexicon.lexicon_name=='Groene Boekje 1995').first()\n",
    "    row = link_df.loc[4554]\n",
    "    wf = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_1']).first()\n",
    "    corr = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_2']).first()\n",
    "    print(wf)\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New notebook to figure this out: [GB_cleaning_problems](../GB_cleaning_problems.ipynb)\n",
    "\n",
    "Ok, figured it out, it was a problem with the wordforms. So now we run again and everything will be fine:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "\n",
    "with ticclat.dbutils.session_scope(Session8) as session:\n",
    "    lexicon = session.query(ticclat.ticclat_schema.Lexicon).filter(ticclat.ticclat_schema.Lexicon.lexicon_name=='Groene Boekje 1995').first()\n",
    "    if lexicon is None:\n",
    "        raise Exception(\"No lexicon found!\")\n",
    "    for idx, row in tqdm.tqdm(link_df.iterrows(), total=link_df.shape[0]):\n",
    "        wf = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_1']).first()\n",
    "        corr = session.query(ticclat.ticclat_schema.Wordform).filter(ticclat.ticclat_schema.Wordform.wordform == row['wordform_2']).first()\n",
    "        if corr is None:\n",
    "            print(row['wordform_2'])\n",
    "        wf.link_with_metadata(corr, True, True, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Put this in the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
