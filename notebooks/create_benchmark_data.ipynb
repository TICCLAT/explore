{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s [%(process)d] %(levelname)-8s \"\n",
    "                    \"%(name)s,%(lineno)s\\t%(message)s\")\n",
    "logging.getLogger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat_test'\n",
    "#db_name = 'ticclat_wikipedia'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus, WordformLink, WordformLinkSource, lexical_source_wordform\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope\n",
    "\n",
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sqlalchemy import select\n",
    "from sqlalchemy import text\n",
    "from sqlalchemy.sql import func, desc\n",
    "\n",
    "num_wordforms = 5\n",
    "\n",
    "vocabulary = {}\n",
    "\n",
    "# select random wordforms\n",
    "with session_scope(Session) as session:\n",
    "    q = select([Wordform]).order_by(func.random()).limit(num_wordforms)\n",
    "    \n",
    "    r = session.execute(q)\n",
    "    for row in r.fetchall():\n",
    "        vocabulary[row['wordform']] = row['wordform_id'] \n",
    "print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "fake = Faker()\n",
    "\n",
    "num_corpora = np.random.randint(3, 10)\n",
    "print('num corpora', num_corpora)\n",
    "\n",
    "for c in range(num_corpora):\n",
    "    corpus_name = 'Corpus {}'.format(i)\n",
    "    num_documents = np.random.randint(10, 100)\n",
    "    print('Corpus {}, {} documents'.format(c, num_documents))\n",
    "    for d in range(num_documents):\n",
    "        num_tokens = np.random.randint(100, 1000)\n",
    "        print('Document {}, {} tokens'.format(d, num_tokens))\n",
    "        \n",
    "        print(fake.words(nb=num_tokens, ext_word_list=vocabulary, unique=False))        \n",
    "        break\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generator that produces documents for a corpus\n",
    "from faker import Faker\n",
    "\n",
    "def random_corpus(num_documents, num_tokens_min, num_tokens_max, vocabulary):\n",
    "    fake = Faker()\n",
    "    for i in range(num_documents):\n",
    "        num_tokens = np.random.randint(num_tokens_min, num_tokens_max)\n",
    "        \n",
    "        yield fake.words(nb=num_tokens, ext_word_list=vocabulary, unique=False)\n",
    "        \n",
    "\n",
    "for doc in random_corpus(10, 10, 15, vocabulary):\n",
    "    print(len(doc))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corpus_metadata(num_documents, language, year_min, year_max):\n",
    "    md = pd.DataFrame()\n",
    "    md['language'] = [language for i in range(num_documents)]\n",
    "    md['pub_year'] = [np.random.randint(year_min, year_max) for i in range(num_documents)]\n",
    "    \n",
    "    return md\n",
    "\n",
    "corpus_metadata(10, 'nl', 2010, 2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.tokenize import terms_documents_matrix_word_lists\n",
    "\n",
    "def generate_corpora(num_corpora, num_documents_min, num_documents_max,\n",
    "                     language, year_min, year_max, num_tokens_min,\n",
    "                     num_tokens_max, vocabulary):\n",
    "    for i in range(num_corpora):\n",
    "        num_documents = np.random.randint(num_documents_min,\n",
    "                                          num_documents_max+1)\n",
    "\n",
    "        md = corpus_metadata(num_documents, language, year_min, year_max+1)\n",
    "\n",
    "        word_lists = random_corpus(num_documents, num_tokens_min,\n",
    "                                   num_tokens_max+1, vocabulary)\n",
    "        corpus, v = terms_documents_matrix_word_lists(word_lists)\n",
    "\n",
    "        yield corpus, v, md\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, v, m in generate_corpora(10, 1, 5, 'nl', 2010, 2015, 1, 5, vocabulary):\n",
    "    print(c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.sacoreutils import add_corpus_core\n",
    "\n",
    "def ingest_corpora(session, num_corpora, num_documents_min, num_documents_max, \n",
    "                     language, year_min, year_max, num_tokens_min, num_tokens_max, vocabulary):\n",
    "    for i, (corpus, v, md) in enumerate(generate_corpora(num_corpora, num_documents_min, num_documents_max,\n",
    "                     language, year_min, year_max, num_tokens_min,\n",
    "                     num_tokens_max, vocabulary)):\n",
    "        name = f'Corpus {i}'\n",
    "        #print(f'Adding {name}')\n",
    "        add_corpus_core(session, corpus_matrix=corpus, vectorizer=v, corpus_name=name, document_metadata=md)\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    ingest_corpora(session, 10, 1, 5, 'nl', 2010, 2015, 1, 5, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.dbutils import add_lexicon\n",
    "\n",
    "def generate_lexica(num_lexica, num_wf_min, num_wf_max, vocabulary):\n",
    "    fake = Faker()\n",
    "    for i in range(num_lexica):\n",
    "        num_wf = np.random.randint(num_wf_min, num_wf_max)\n",
    "        \n",
    "        wfs = pd.DataFrame()\n",
    "        wfs['wordform'] = fake.words(nb=num_wf, ext_word_list=vocabulary, unique=True)\n",
    "        \n",
    "        yield wfs\n",
    "        \n",
    "def ingest_lexica(session, num_lexica, num_wf_min, num_wf_max, vocabulary):\n",
    "    lexica = generate_lexica(num_lexica, num_wf_min, num_wf_max+1, vocabulary)\n",
    "    for i, wfs in enumerate(lexica):\n",
    "        name = f'Lexicon {i}'\n",
    "        add_lexicon(session, lexicon_name=name, vocabulary=True, wfs=wfs)\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    ingest_lexica(session, num_lexica=3, num_wf_min=1, num_wf_max=3, vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_linked_lexica(num_lexica, num_wf_min, num_wf_max, vocabulary):\n",
    "    fake = Faker()\n",
    "    for i in range(num_lexica):\n",
    "        num_wf = np.random.randint(num_wf_min, num_wf_max)\n",
    "        if num_wf % 2 != 0:\n",
    "            num_wf += 1\n",
    "        print('num wordforms', num_wf)\n",
    "        \n",
    "        words = fake.words(nb=num_wf, ext_word_list=vocabulary, unique=True)\n",
    "\n",
    "        wfs = pd.DataFrame()\n",
    "        wfs['from'] = words[:num_wf/2]\n",
    "        wfs['to'] = words[num_wf/2:]\n",
    "        yield wfs\n",
    "        \n",
    "def ingest_lexica(session, num_lexica, num_wf_min, num_wf_max, vocabulary):\n",
    "    lexica = generate_linked_lexica(num_lexica, num_wf_min, num_wf_max+1, vocabulary)\n",
    "    for i, wfs in enumerate(lexica):\n",
    "        name = f'Lexicon {i}'\n",
    "        #add_lexicon(session, lexicon_name=name, vocabulary=True, wfs=wfs)\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    ingest_lexica(session, num_lexica=3, num_wf_min=1, num_wf_max=3, vocabulary=vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    print('number of wordforms:', session.query(Wordform).count())\n",
    "    print('number of lexica:', session.query(Lexicon).count())\n",
    "    print('number of documents:', session.query(Document).count())\n",
    "    print('number of corpora:', session.query(Corpus).count())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
