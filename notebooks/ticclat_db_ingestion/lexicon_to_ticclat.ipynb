{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('../ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()\n",
    "            \n",
    "os.environ['lexicon_name'] = os.environ['dbname']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import MySQLdb\n",
    "import textwrap\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def do_query(query):\n",
    "    cn = MySQLdb.connect(host='localhost', \n",
    "                         port=3306,\n",
    "                         user=os.environ.get('user'), \n",
    "                         passwd=os.environ.get('password'),\n",
    "                         db=os.environ.get('dbname'))\n",
    "    df_mysql = pd.read_sql(query, con=cn)    \n",
    "    cn.close()\n",
    "    # deduplicate columns\n",
    "    df_mysql = df_mysql.loc[:,~df_mysql.columns.duplicated()]\n",
    "    return df_mysql\n",
    "\n",
    "tables = do_query('SHOW TABLES;')\n",
    "tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs = do_query('SELECT * FROM wordforms;')\n",
    "wfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wfs.shape)\n",
    "print(len(wfs['wordform'].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some duplicate wordforms. This is a violation of the uniqueness constraint on wordform. So, we need to filter the dataframe before adding it to the database.\n",
    "\n",
    "Also `has_analysis` is set to `False`, because we might be using `bulk_save_objects`, which doesn't set defaults."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs = wfs.drop_duplicates(subset='wordform')\n",
    "wfs['has_analysis'] = False\n",
    "wfs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as frequency list for ticcl\n",
    "wfs['freq'] = 1\n",
    "wfs.head()\n",
    "wfs.to_csv(os.environ['lexicon_name'], sep='\\t', header=False, index=False, columns=['wordform', 'freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    lex = Lexicon(lexicon_name=os.environ['lexicon_name'])\n",
    "    wf = Wordform(wordform_id=528954, \n",
    "                  wordform='tuyld',\n",
    "                  has_analysis=False,\n",
    "                  wordform_lowercase='tuyld')\n",
    "    wf.lexica.append(lex)\n",
    "    session.add(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    a = Anahash(anahash=46901904807)\n",
    "    session.add(a)\n",
    "    wf = Wordform(wordform='uit',\n",
    "                  has_analysis=False,\n",
    "                  wordform_lowercase='uit')\n",
    "    wf.anahash = a\n",
    "    session.add(wf)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    print('number of wordforms:', session.query(Wordform).count())\n",
    "    print('number of lexica:', session.query(Lexicon).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import get_or_create_wordform\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    #lex = Lexicon(lexicon_name=os.environ['lexicon_name'])\n",
    "    # We can't use apply, because apply calls the function twice for the first row, see\n",
    "    # http://pandas.pydata.org/pandas-docs/stable/groupby.html#flexible-apply\n",
    "    for idx, row in tqdm(wfs.iterrows(), total=wfs.shape[0]):\n",
    "        #print(idx)\n",
    "        wf = get_or_create_wordform(session, row['wordform'], has_analysis=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import bulk_add_wordforms\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    n = bulk_add_wordforms(session, wfs, num=10000)\n",
    "print('added {} wordforms'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import add_lexicon\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    add_lexicon(session, os.environ['lexicon_name'], wfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should give a single result!\n",
    "with session_scope(Session) as session:\n",
    "    data = session.query(Wordform).filter(Wordform.wordform == 'dóór').all()\n",
    "    for wf in data:\n",
    "        print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.dbutils import get_word_frequency_df\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    df = get_word_frequency_df(session)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "hashes = pd.read_csv('{}.clean.list'.format(os.environ['lexicon_name']), \n",
    "                     sep='\\t', \n",
    "                     header=None, \n",
    "                     keep_default_na=False)  # make sure word 'null' is read as string and not NaN\n",
    "hashes.columns = ['wordform', 'anahash']\n",
    "hashes = hashes.set_index('wordform', verify_integrity=True)\n",
    "print(hashes.shape)\n",
    "hashes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import bulk_add_anahashes\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    total = bulk_add_anahashes(session, hashes)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import connect_anahases_to_wordforms\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    total = connect_anahases_to_wordforms(session, hashes)\n",
    "print(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    wfs = session.query(Wordform).filter(Wordform.anahash_id == None).all()\n",
    "print(len(wfs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.dbutils import get_word_frequency_df\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    df = get_word_frequency_df(session)\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many wordforms have not been assigned an anahash value. Maybe not clean the frequency list?"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
