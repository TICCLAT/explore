{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Groene boekje\n",
    "* Use `dbutils.add_lexicon`!\n",
    "* Use `bulk_add_anahashes` and `connect_anahases_to_wordforms`\n",
    "\n",
    "INL takse about 3 minutes (without anahashes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ticclat.dbutils\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do\n",
    "\n",
    "* Add relationships between models (should make processing an xml file faster)\n",
    "* Use sessions better: https://docs.sqlalchemy.org/en/latest/orm/session_basics.html#when-do-i-construct-a-session-when-do-i-commit-it-and-when-do-i-close-it\n",
    "* Add multiple documents\n",
    "* Extract vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "from sqlalchemy_utils import database_exists, create_database\n",
    "\n",
    "engine = create_engine(\"mysql://{}:{}@localhost/{}\".format(os.environ['user'], \n",
    "                                                           os.environ['password'], \n",
    "                                                           os.environ['dbname']))\n",
    "if not database_exists(engine.url):\n",
    "    create_database(engine.url)\n",
    "\n",
    "print(database_exists(engine.url))\n",
    "\n",
    "Session = sessionmaker(bind=engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ticclat.lexicon_schema import AnalyzedWordform, Document, Lemmata, TokenAttestation, Wordform, Base\n",
    "import ticclat.ticclat_schema as schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tables\n",
    "schema.Base.metadata.create_all(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import inspect\n",
    "\n",
    "inspector = inspect(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get table information\n",
    "print(inspector.get_table_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Groene Boekje data into Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB_basepath = \"/Users/pbos/projects/ticclat/data/GB/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GB1914_path = GB_basepath + \"1914/22722-8.txt\"\n",
    "GB1995_path = GB_basepath + \"1995-2005/1995/GB95_002.csv\"\n",
    "GB2005_path = GB_basepath + \"1995-2005/2005/GB05_002.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995 = pd.read_csv(GB1995_path, sep=';', names=[\"word\", \"syllables\", \"see also\", \"disambiguation\",\n",
    "                                                     \"grammatical tag\", \"article\",\n",
    "                                                     \"plural/past/attrib\", \"plural/past/attrib syllables\",\n",
    "                                                     \"diminu/compara/past plural\", \"diminu/compara/past plural syllables\",\n",
    "                                                     \"past perfect/superla\", \"past perfect/superla syllables\"],\n",
    "                        encoding='utf8') # encoding necessary for later loading into sqlalchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_GB1995\n",
    "# df_GB1995['see also'].dropna().map(lambda x: x[:8]).unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There's a lot of stuff in there... clean up time.\n",
    "\n",
    "Multiple columns:\n",
    "- Some entries have \"@\" in them. They seem to be general rules for some kinds of words. Not sure what to do with these, so will just filter out for now.\n",
    "\n",
    "First column:\n",
    "- Has entries with multiple values (e.g. \"Zwols, Zwoller\")\n",
    "- Sometimes has multiple rows with the same word, probably in different meanings or something, e.g. \"aal1\", \"aal2\" and \"aal3\".\n",
    "    + In this case, the fourth column has a note to disambiguate the meanings.\n",
    "\n",
    "Second column: Splits words according to syllables, we can disregard this.\n",
    "\n",
    "Third: a \"See also \\[other word\\]\" note (they all start with \"Zie ook\")\n",
    "\n",
    "Fourth: disambiguation of a duplicate word (see first column)\n",
    "\n",
    "Fifth: grammatical tag, i.e. noun, adjective, verb, etc.\n",
    "\n",
    "Sixth: the proper definite article for the noun (\"de\" or \"het\").\n",
    "\n",
    "Seventh: first inflection form. Sometimes left empty.\n",
    "- If a noun: plural form\n",
    "- If a verb: past tense singular\n",
    "- If an adjective (or pronoun (`vnw.`)): attributive form\n",
    "\n",
    "Nineth: second inflection form. Often left empty, probably for regular or easy forms.\n",
    "- If a noun: diminutive\n",
    "- If an adjective: comparative\n",
    "- If a verb: past tense plural\n",
    "\n",
    "Eleventh:\n",
    "- If a verb: past perfect (voltooid verleden tijd?)\n",
    "- If an adjective: superlative\n",
    "\n",
    "8, 10, 12: Syllables of the preceding.\n",
    "\n",
    "## Clean up\n",
    "\n",
    "### Wordforms\n",
    "- 1 needs most work:\n",
    "  + Split up if it has a comma and two (or more) words --> Make it two (or more) rows; check that the other columns also have **the same number** of comma separated words and match those, otherwise just duplicate other columns.\n",
    "  + Make a new column for rows with a \"duplicate entry number\" postfixed; put the number there, remove it from the first row.\n",
    "    - Note that column 3 also uses the postfix number to refer to specific duplicates!\n",
    "- 2, 8, 10 and 12 can go, we have no need for pronunciation information currently.\n",
    "- We have no place in the current schema for 5, so we drop it as well for now.\n",
    "- Column 3 is for links only.\n",
    "- Columns 7, 9 and 11 also contain wordforms, so we give these their own rows as well. For now, we don't need links, so we just extract the words and worry about efficient linking later.\n",
    "\n",
    "### Links\n",
    "This lexicon also contains a lot of word links.\n",
    "\n",
    "- Obviously, the three inflection columns, 7 9 and 11, give clear immediate links between wordforms, i.e. grammatical ones.\n",
    "- Column 3 also gives a link, of words with similar meanings, i.e. a semantic link.\n",
    "\n",
    "### Grammatical function\n",
    "Column 5 may at some point be used to add grammatical information, if we ever plan on using this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actual wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_wordforms = df_GB1995.drop([\"syllables\", \"see also\", \"disambiguation\", \"grammatical tag\", \"article\",\n",
    "                                      \"plural/past/attrib syllables\", \"diminu/compara/past plural syllables\", \"past perfect/superla syllables\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_wordforms.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995[\"grammatical tag\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995[df_GB1995[\"grammatical tag\"] == 'tw.']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should probably skip `uitdr.` (sayings). Maybe also `eigenn.` (proper names). Will leave them in for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wordform clean up\n",
    "\n",
    "## Mysterious \"@\" rows\n",
    "Let's take out the weird \"@\" rows first to see what we can do with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nd_any(*args):\n",
    "    result = (args[0] | args[0])\n",
    "    for arg in args:\n",
    "        result = (result | arg)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_at = df_GB1995[nd_any(*tuple(df_GB1995[col].str.contains('@') for col in df_GB1995.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_at.sample(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = df_GB1995_at.values.flatten()\n",
    "x = pd.Series(x).dropna().to_frame()\n",
    "x = x[x[0].str.contains('@')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ats = x[0].str.extract('(\\@[^\\w]*)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ats[0].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhh, they seem to be diacritic markers!\n",
    "- @\\` is accent grave on the previous character\n",
    "- @\\' is accent aigu on the previous character\n",
    "- @@ seems to be apostrophe\n",
    "\n",
    "But then... a whole zoo of quite rare ones. Let's check out examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for at in ats[0].unique():\n",
    "    print(x[ats[0].str.contains(at.replace('\\\\', \"\\\\\\\\\"))].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the three-character ones seem to be wrong: / are all just syllable separators, with a - behind them are just koppeltekens, with space is just space, etc., so let's cut it down to a two-character filter (actually also just one, there's a naked @ as well... though is that one actually naked or does the following character there also have special meaning? **make sure**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ats = x[0].str.extract('(\\@[^\\w]?)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ats[0].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for at in ats[0].unique():\n",
    "    print(at)\n",
    "    print(x[ats[0].str.contains(at.replace('\\\\', \"\\\\\\\\\").replace('+', \"\\+\").replace('^', \"\\^\"))].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- @\\` is accent grave on the previous character\n",
    "- @\\' is accent aigu\n",
    "- @\\\\ is trema\n",
    "- @+ is cedilla\n",
    "- @^ is accent circumflex\n",
    "- @= is a tilde\n",
    "- @@ is apostrophe between the characters\n",
    "\n",
    "The naked @ requires a bit more work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naked_at = x[~nd_any(*tuple(ats[0].str.contains(at.replace('\\\\', \"\\\\\\\\\").replace('+', \"\\+\").replace('^', \"\\^\")) for at in ats[0].unique() if at != '@'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "naked_at[~naked_at[0].str.contains('/')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so the presumed naked @ is actually two possible things I wrongly filtered out:\n",
    "- @2 is 2 in subscript, as in CO$_2$ (and only that)\n",
    "- @n comes exclusively before an e that should have a trema on it; the backslash that's normally used is probably omitted to avoid confusion with newline-character \\\\n\n",
    "\n",
    "We will normalize the diacritics below when we have gathered all wordforms in one array, for easier processing.\n",
    "\n",
    "## Gather wordforms\n",
    "\n",
    "Now to gather all actual wordforms, i.e. those in columns 1, 7, 9 and 11, also splitting by comma."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = pd.concat((df_GB1995[\"word\"], df_GB1995[df_GB1995.columns[6]], df_GB1995[df_GB1995.columns[8]], df_GB1995[df_GB1995.columns[10]]))\\\n",
    "                .dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(', ')].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(':')].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ffs, some words have colons in them, what do those mean then?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_colon = df_GB1995[nd_any(*tuple(df_GB1995[col].str.contains(':') for col in df_GB1995.columns))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995_colon.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have no idea what this means, so will just filter it out.\n",
    "\n",
    "Also, there are \"words\" that are actually several words... arrggh!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_comma = wordform_df.str.contains(', ')\n",
    "wordform_df = pd.concat((wordform_df[~has_comma],) + tuple(pd.Series(row.split(', ')) for row in wordform_df[has_comma]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(', ')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out crap\n",
    "- Colons behind some words: we remove the colons and leave the rest of the word in.\n",
    "- Words that are in fact several words, like \"aan weerszijden\" or \"schoof vooruit\". Remove those entries, split the words and append them to the end.\n",
    "- Strip whitespace from either end of words (some apparently have it).\n",
    "- Remove \"footnote\" numbers postfixed to duplicate words: like with colons.\n",
    "- Retain only unique words after the above procedures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# draw some samples to check for remaining weird shit\n",
    "wordform_df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove colons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = wordform_df.str.replace(':', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(':')].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split multiple word entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# surround space with any-character, because some single words also just have space padding\n",
    "multi_word = wordform_df.str.contains('. .', regex=True)\n",
    "wordform_df = pd.concat((wordform_df[~multi_word],) + tuple(pd.Series(row.split(' ')) for row in wordform_df[multi_word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains('. .')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strip whitespace\n",
    "Ok, so it's actually just one word, but still."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(' ')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = wordform_df.str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains(' ')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove duplicate word footnote numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = wordform_df.str.contains('[0-9]$', regex=True)\n",
    "wordform_df = pd.concat((wordform_df[~duplicates], wordform_df[duplicates].str.replace('[0-9]$', '', regex=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df.tail(), wordform_df[wordform_df.str.contains('[0-9]$', regex=True)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Retain only unique wordforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = pd.Series(wordform_df.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df.sort_values().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra: remove stuff between parentheses\n",
    "This turned up when sorting. We should remove \"etc.\", which is a abbreviation, but we could keep the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = wordform_df.sort_values().str.strip(\"()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra 2: abbreviations\n",
    "Are there any more of them?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains('.', regex=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, we remove the ones that are \"pure\" abbreviations, i.e. that end in a period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abbreviation = wordform_df.str.contains('\\.$')\n",
    "wordform_df = wordform_df[~abbreviation]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df[wordform_df.str.contains('.', regex=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extra 3: Retain only unique wordforms... again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = pd.Series(wordform_df.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df.sort_values().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is a problem. \"'s\" is not a separate word, it really belongs to some other word, that we split it off from.\n",
    "\n",
    "### Aaaand again\n",
    "\n",
    "After discussion, we decided to keep in the multi-word wordforms after all. We will see how to deal with them in TICCL later.\n",
    "\n",
    "For the occasion, let's also just put everything in one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = pd.concat((df_GB1995[\"word\"],\n",
    "                         df_GB1995[\"plural/past/attrib\"],\n",
    "                         df_GB1995[\"diminu/compara/past plural\"],\n",
    "                         df_GB1995[\"past perfect/superla\"]))\\\n",
    "                .dropna()\n",
    "has_comma = wordform_df.str.contains(', ')\n",
    "wordform_df = pd.concat((wordform_df[~has_comma],) + tuple(pd.Series(row.split(', ')) for row in wordform_df[has_comma]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_wordform_df(wordform_df):\n",
    "    # remove colons\n",
    "    wordform_df = wordform_df.str.replace(':', '')\n",
    "    # strip whitespace\n",
    "    wordform_df = wordform_df.str.strip()\n",
    "    # remove duplicate word footnote numbers\n",
    "    duplicates = wordform_df.str.contains('[0-9]$', regex=True)\n",
    "    wordform_df = pd.concat((wordform_df[~duplicates], wordform_df[duplicates].str.replace('[0-9]$', '', regex=True)))\n",
    "    # remove parentheses around some words\n",
    "    wordform_df = wordform_df.sort_values().str.strip(\"()\")\n",
    "    # remove abbreviations\n",
    "    abbreviation = wordform_df.str.contains('\\.$')\n",
    "    wordform_df = wordform_df[~abbreviation]\n",
    "    # remove duplicates\n",
    "    wordform_df = pd.Series(wordform_df.unique())\n",
    "    return wordform_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_clean_df = clean_wordform_df(wordform_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_cleanliness_wordform_df(wordform_df, head=5):\n",
    "    print(\"Random sample:\")\n",
    "    display(wordform_df.sample(10))\n",
    "    print(\"Colons, periods:\")\n",
    "    display(wordform_df[wordform_df.str.contains(':')].head(head))\n",
    "    display(wordform_df[wordform_df.str.contains('.', regex=False)].head(head))\n",
    "    print(\"White space padding:\")\n",
    "    display(wordform_df[wordform_df.str.contains('^ | $')].head(head))\n",
    "    print(\"Trailing numbers:\")\n",
    "    display(wordform_df[wordform_df.str.contains('[0-9]$', regex=True)].head(head))\n",
    "    print(\"Parentheses:\")\n",
    "    display(wordform_df[wordform_df.str.contains('\\(|\\)', regex=True)].head(head))\n",
    "    print(\"Abbreviations:\")\n",
    "    display(wordform_df[wordform_df.str.contains('\\.$')].head(head))\n",
    "    \n",
    "    print(\"Finally, just the first entries of sorted df:\")\n",
    "    display(wordform_df.sort_values().head(head))\n",
    "    display(wordform_df.sort_values().tail(head))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cleanliness_wordform_df(wordform_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_cleanliness_wordform_df(wordform_clean_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, there's some remaining issues here that should be at least duly noted (though ideally dealt with):\n",
    "\n",
    "- Some words have `(etc.)` in it, like `ten tweede (etc.) male`. This should be expanded into `ten derde male`, and so forth for all counting words.\n",
    "- Other parenthesized words like `op de(n) duur` should be considered as two words, `op de duur` and `op den duur`.\n",
    "- I'm not sure what the period in `vademen. vadems` means, though I suspect the period is a mistyped comma.\n",
    "\n",
    "**We will leave these things as they are for now.**\n",
    "\n",
    "One more pressing remaining issue is that there are apparently \"multiple words\" that are not separated by comma at all! See `zworen af zweerde af` which should be two terms. Let's check what this looks like in the original table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995[df_GB1995['word'].str.contains('zweren', na=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No idea how to fix this... **yet**. Leave it as is for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalizing diacritics\n",
    "\n",
    "TICCLAT has unicode wordforms, so we can just replace the diacritic markers with actual diacritics. How to do that?\n",
    "\n",
    "Apparently, there is such a thing as \"combining characters\" in Unicode: https://stackoverflow.com/questions/34755556/how-do-i-add-accents-to-a-letter. Nice! Here's a table of them: https://en.wikipedia.org/wiki/Combining_character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that these are regex formatted, i.e. with special characters escaped\n",
    "diacritic_markers = {'@`': '\\u0300',    # accent grave\n",
    "                     \"@\\\\'\": '\\u0301',  # accent aigu\n",
    "                     '@\\\\\\\\': '\\u0308', # trema\n",
    "                     '@\\+': '\\u0327',   # cedilla\n",
    "                     '@\\^': '\\u0302',   # accent circumflex\n",
    "                     '@=': '\\u0303',    # tilde\n",
    "                     '@@': \"'\",         # apostrophe (not actually a diacritic)\n",
    "                     '@2': '\\u2082',    # subscript 2\n",
    "                     '@n': '\\u0308n'    # trema followed by n\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for marker, umarker in diacritic_markers.items():\n",
    "    wordform_clean_df = wordform_clean_df.str.replace(marker, umarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_clean_df.sort_values().head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Groene Boekje DataFrames into TICCLAT database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ticclat.dbutils.session_scope(Session) as session:\n",
    "    ticclat.dbutils.add_lexicon(session, \"Groene Boekje 1995\", wordform_clean_df.to_frame(name='wordform'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, this is giving me an error\n",
    "\n",
    "```python-traceback\n",
    "---------------------------------------------------------------------------\n",
    "UnicodeEncodeError                        Traceback (most recent call last)\n",
    "<ipython-input-93-22e27abe721c> in <module>\n",
    "      1 with ticclat.dbutils.session_scope(Session) as session:\n",
    "----> 2     ticclat.dbutils.add_lexicon(session, \"Groene Boekje 1995\", wordform_clean_df.to_frame(name='wordform'))\n",
    "\n",
    "~/projects/ticclat/ticclat/ticclat/dbutils.py in add_lexicon(session, lexicon_name, wfs, num)\n",
    "     97     in this case just \"wordform\"\n",
    "     98     \"\"\"\n",
    "---> 99     bulk_add_wordforms(session, wfs, num=num)\n",
    "    100 \n",
    "    101     lexicon = Lexicon(lexicon_name=lexicon_name)\n",
    "\n",
    "~/projects/ticclat/ticclat/ticclat/dbutils.py in bulk_add_wordforms(session, wfs, num)\n",
    "     72 \n",
    "     73         q = session.query(Wordform)\n",
    "---> 74         result = q.filter(Wordform.wordform.in_(wordforms)).all()\n",
    "     75 \n",
    "     76         existing_wfs = [wf.wordform for wf in result]\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/orm/query.py in all(self)\n",
    "   2923 \n",
    "   2924         \"\"\"\n",
    "-> 2925         return list(self)\n",
    "   2926 \n",
    "   2927     @_generative(_no_clauseelement_condition)\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/orm/query.py in __iter__(self)\n",
    "   3079         if self._autoflush and not self._populate_existing:\n",
    "   3080             self.session._autoflush()\n",
    "-> 3081         return self._execute_and_instances(context)\n",
    "   3082 \n",
    "   3083     def __str__(self):\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/orm/query.py in _execute_and_instances(self, querycontext)\n",
    "   3104         )\n",
    "   3105 \n",
    "-> 3106         result = conn.execute(querycontext.statement, self._params)\n",
    "   3107         return loading.instances(querycontext.query, result, querycontext)\n",
    "   3108 \n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/base.py in execute(self, object_, *multiparams, **params)\n",
    "    978             raise exc.ObjectNotExecutableError(object_)\n",
    "    979         else:\n",
    "--> 980             return meth(self, multiparams, params)\n",
    "    981 \n",
    "    982     def _execute_function(self, func, multiparams, params):\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/sql/elements.py in _execute_on_connection(self, connection, multiparams, params)\n",
    "    271     def _execute_on_connection(self, connection, multiparams, params):\n",
    "    272         if self.supports_execution:\n",
    "--> 273             return connection._execute_clauseelement(self, multiparams, params)\n",
    "    274         else:\n",
    "    275             raise exc.ObjectNotExecutableError(self)\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_clauseelement(self, elem, multiparams, params)\n",
    "   1097             distilled_params,\n",
    "   1098             compiled_sql,\n",
    "-> 1099             distilled_params,\n",
    "   1100         )\n",
    "   1101         if self._has_events or self.engine._has_events:\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n",
    "   1238         except BaseException as e:\n",
    "   1239             self._handle_dbapi_exception(\n",
    "-> 1240                 e, statement, parameters, cursor, context\n",
    "   1241             )\n",
    "   1242 \n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _handle_dbapi_exception(self, e, statement, parameters, cursor, context)\n",
    "   1458                 util.raise_from_cause(sqlalchemy_exception, exc_info)\n",
    "   1459             else:\n",
    "-> 1460                 util.reraise(*exc_info)\n",
    "   1461 \n",
    "   1462         finally:\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/util/compat.py in reraise(tp, value, tb, cause)\n",
    "    275         if value.__traceback__ is not tb:\n",
    "    276             raise value.with_traceback(tb)\n",
    "--> 277         raise value\n",
    "    278 \n",
    "    279 \n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/base.py in _execute_context(self, dialect, constructor, statement, parameters, *args)\n",
    "   1234                 if not evt_handled:\n",
    "   1235                     self.dialect.do_execute(\n",
    "-> 1236                         cursor, statement, parameters, context\n",
    "   1237                     )\n",
    "   1238         except BaseException as e:\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/sqlalchemy/engine/default.py in do_execute(self, cursor, statement, parameters, context)\n",
    "    534 \n",
    "    535     def do_execute(self, cursor, statement, parameters, context=None):\n",
    "--> 536         cursor.execute(statement, parameters)\n",
    "    537 \n",
    "    538     def do_execute_no_params(self, cursor, statement, context=None):\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/MySQLdb/cursors.py in execute(self, query, args)\n",
    "    237                 args = dict((key, db.literal(item)) for key, item in args.items())\n",
    "    238             else:\n",
    "--> 239                 args = tuple(map(db.literal, args))\n",
    "    240             if not PY2 and isinstance(query, (bytes, bytearray)):\n",
    "    241                 query = query.decode(db.encoding)\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/MySQLdb/connections.py in literal(self, o)\n",
    "    319             s = self._tuple_literal(o)\n",
    "    320         else:\n",
    "--> 321             s = self.escape(o, self.encoders)\n",
    "    322         # Python 3(~3.4) doesn't support % operation for bytes object.\n",
    "    323         # We should decode it before using %.\n",
    "\n",
    "~/sw/miniconda3/envs/ticclat2/lib/python3.7/site-packages/MySQLdb/connections.py in unicode_literal(u, dummy)\n",
    "    227             # unicode_literal() is called for arbitrary object.\n",
    "    228             def unicode_literal(u, dummy=None):\n",
    "--> 229                 return db.string_literal(str(u).encode(db.encoding))\n",
    "    230 \n",
    "    231         def bytes_literal(obj, dummy=None):\n",
    "\n",
    "UnicodeEncodeError: 'latin-1' codec can't encode character '\\u0308' in position 14: ordinal not in range(256)\n",
    "```\n",
    "\n",
    "It seems like the `db.encoding` for some reason is latin-1 there, even though we set the ticclat database to be utf8 in MySQL using\n",
    "\n",
    "```mysql\n",
    "CREATE DATABASE ticclat CHARACTER SET utf8mb4 COLLATE utf8mb4_bin;\n",
    "```\n",
    "\n",
    "This is a guide that should set everything to utf8: https://mathiasbynens.be/notes/mysql-utf8mb4#mysql-utf8mb4\n",
    "\n",
    "The important part there that we're still missing is that `character_set_client` may still be non-utf8, i.e. clients could still read the data as latin-1, even though the database is encoded as utf8 (see e.g. https://nicj.net/mysql-converting-an-incorrect-latin1-column-to-utf8/). In my case it seems like its neither latin-1 nor utf8mb4, but actually \"utf8\" which is an alias for utf8mb3 (see https://stackoverflow.com/a/30074553/1199693), as I found out by running `SHOW VARIABLES LIKE 'character_set_client';`.\n",
    "\n",
    "We will do this by creating a configuration file. To find out which file your client reads from, run this:\n",
    "```sh\n",
    "mysql --help | grep -A 1 \"Default options are read from the following files\"\n",
    "```\n",
    "\n",
    "For me this gives\n",
    "```sh\n",
    "Default options are read from the following files in the given order:\n",
    "/etc/my.cnf /etc/mysql/my.cnf /Users/pbos/sw/miniconda3/envs/ticclat2/etc/my.cnf ~/.my.cnf\n",
    "```\n",
    "\n",
    "I'm using MySQL from Conda, so I'll use the miniconda location and put this in the file:\n",
    "\n",
    "```ini\n",
    "[client]\n",
    "default-character-set = utf8mb4\n",
    "\n",
    "[mysql]\n",
    "default-character-set = utf8mb4\n",
    "\n",
    "[mysqld]\n",
    "character-set-client-handshake = FALSE\n",
    "character-set-server = utf8mb4\n",
    "collation-server = utf8mb4_unicode_ci\n",
    "```\n",
    "\n",
    "Then restart the server and recreate the database (don't care much about saving data at this point) by deleting and recreating.\n",
    "\n",
    "```sh\n",
    "mysqld restart\n",
    "```\n",
    "\n",
    "Indeed, now `SHOW VARIABLES LIKE 'character_set_client';` gives utf8mb4.\n",
    "\n",
    ".......\n",
    "\n",
    "Ok, never mind, I was using the \"groene_boekje\" database, not the \"ticclat\" database. D'oh!\n",
    "\n",
    ".......\n",
    "\n",
    "Ok, but with that fixed, the problem still persists. Even with the whole configuration file above added... crap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ticclat.dbutils.session_scope(Session) as session:\n",
    "    ticclat.dbutils.add_lexicon(session, \"Groene Boekje 1995\", wordform_clean_df.str.encode('utf8').to_frame(name='wordform'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wordform_clean_df.to_frame(name='wordform')['wordform']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yaaay, that seems to have worked! The crucial addition there is `.str.encode('utf-8')`. Actually, we already discussed this, but I got lost in the MySQL settings and forgot.\n",
    "\n",
    "This is also useful to check the encoding of tables themselves:\n",
    "```mysql\n",
    "SELECT T.table_name, CCSA.character_set_name FROM information_schema.`TABLES` T,\n",
    "information_schema.`COLLATION_CHARACTER_SET_APPLICABILITY` CCSA\n",
    "WHERE CCSA.collation_name = T.table_collation AND T.table_schema = \"ticclat\";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But also have to make sure that the data is put in correctly. Just browsing through the data in mysql with\n",
    "\n",
    "```mysql\n",
    "SELECT * FROM wordforms;\n",
    "```\n",
    "\n",
    "Shows that the unicode words are not displayed correctly in the terminal at least, e.g. `zooÌˆloog\"` instead of `zoöloog`.\n",
    "\n",
    "How about if we query from here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'zoo{}loog'.format(diacritic_markers['@\\\\\\\\'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again gives long utf-8/latin-1 UnicodeEncodeError:\n",
    "# with ticclat.dbutils.session_scope(Session) as session:\n",
    "#     for wordform in session.query(ticclat.ticclat_schema.Wordform).filter_by(wordform='zoo{}loog'.format(diacritic_markers['@\\\\\\\\'])):\n",
    "#          print(wordform)\n",
    "\n",
    "# this gives no results at all\n",
    "with ticclat.dbutils.session_scope(Session) as session:\n",
    "    for wordform in session.query(ticclat.ticclat_schema.Wordform).filter_by(wordform='zooÌˆloog\"'.encode('utf-8')):\n",
    "         print(wordform)\n",
    "\n",
    "# ... neither does this, when I manually type in zoöloog with alt-U O (on macOS) for the ö\n",
    "with ticclat.dbutils.session_scope(Session) as session:\n",
    "    for wordform in session.query(ticclat.ticclat_schema.Wordform).filter_by(wordform='zoöloog\"'.encode('utf-8')):\n",
    "         print(wordform)\n",
    "\n",
    "# ... but when I copy-paste the output of the cell above it works!\n",
    "with ticclat.dbutils.session_scope(Session) as session:\n",
    "    for wordform in session.query(ticclat.ticclat_schema.Wordform).filter_by(wordform='zoöloog\"'.encode('utf-8')):\n",
    "         print(wordform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll leave this unicode retrieval problem for later (made an issue).\n",
    "\n",
    "However, in the links notebook we ran into a related issue, which are the double quotes at the ends of these words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if '\"' in w][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This actually seems to be a bigger problem! In some cases, there are apparently rows where cells have been misidentified, like `'België\";Bel/gië\"\"'` above. This is really a mystery; why is that `;` not identified as a separator in the csv? Is it really a different unicode character?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if '\"' in w][10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if '\"' in w][10][8] == ';'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nope, just a regular `;`... Let's see what this line looks like in the file, here's the paste from grep:\n",
    "\n",
    "```\n",
    "\"Belgie@\\\"\";\"Bel/gie@\\\"\";;;\"eigenn.\";;;;;;;\n",
    "```\n",
    "\n",
    "AHA, I've been identifying trema incorrectly! It's not `@\\` as I assumed, it's `@\\\"`, which makes way more sense anyway. Also, are those escaped double quotes causing some issues?\n",
    "\n",
    "Let's add `escapechar` for starters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995 = pd.read_csv(GB1995_path, sep=';', escapechar='\\\\',\n",
    "                        names=[\"word\", \"syllables\", \"see also\", \"disambiguation\",\n",
    "                               \"grammatical tag\", \"article\",\n",
    "                               \"plural/past/attrib\", \"plural/past/attrib syllables\",\n",
    "                               \"diminu/compara/past plural\", \"diminu/compara/past plural syllables\",\n",
    "                               \"past perfect/superla\", \"past perfect/superla syllables\"],\n",
    "                        encoding='utf8') # encoding necessary for later loading into sqlalchemy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that these are regex formatted, i.e. with special characters escaped\n",
    "diacritic_markers = {'@`': '\\u0300',    # accent grave\n",
    "                     \"@\\\\'\": '\\u0301',  # accent aigu\n",
    "                     '@\\\\\"': '\\u0308', # trema\n",
    "                     '@\\+': '\\u0327',   # cedilla\n",
    "                     '@\\^': '\\u0302',   # accent circumflex\n",
    "                     '@=': '\\u0303',    # tilde\n",
    "                     '@@': \"'\",         # apostrophe (not actually a diacritic)\n",
    "                     '@2': '\\u2082',    # subscript 2\n",
    "                     '@n': '\\u0308n'    # trema followed by n\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordform_df = pd.concat((df_GB1995[\"word\"],\n",
    "                         df_GB1995[\"plural/past/attrib\"],\n",
    "                         df_GB1995[\"diminu/compara/past plural\"],\n",
    "                         df_GB1995[\"past perfect/superla\"]))\\\n",
    "                .dropna()\n",
    "has_comma = wordform_df.str.contains(', ')\n",
    "wordform_df = pd.concat((wordform_df[~has_comma],) + tuple(pd.Series(row.split(', ')) for row in wordform_df[has_comma]))\n",
    "\n",
    "wordform_clean_df = clean_wordform_df(wordform_df)\n",
    "\n",
    "for marker, umarker in diacritic_markers.items():\n",
    "    wordform_clean_df = wordform_clean_df.str.replace(marker, umarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if '\"' in w], [w for w in wordform_clean_df if ';' in w], [w for w in wordform_clean_df if '/' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if 'Belgi' in w]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's better! Except that Ilias... The grep:\n",
    "\n",
    "```\n",
    "\"Ilias\";;;\"znw.\";\"de\";;\"Ili/as\";;;;;\"5.2.5;5.3.5a\"\n",
    "```\n",
    "\n",
    "Ah, there's a missing `;` after the first cell, really weird line, the syllables cell which should be second is then in the place of the plural/past/attrib cell... ok, so we clean that one manually and we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_GB1995 = pd.read_csv(GB1995_path, sep=';', escapechar='\\\\',\n",
    "                        names=[\"word\", \"syllables\", \"see also\", \"disambiguation\",\n",
    "                               \"grammatical tag\", \"article\",\n",
    "                               \"plural/past/attrib\", \"plural/past/attrib syllables\",\n",
    "                               \"diminu/compara/past plural\", \"diminu/compara/past plural syllables\",\n",
    "                               \"past perfect/superla\", \"past perfect/superla syllables\"],\n",
    "                        encoding='utf8') # encoding necessary for later loading into sqlalchemy!\n",
    "\n",
    "df_GB1995 = df_GB1995.where(df_GB1995 != \"Ili/as\", other=None)\n",
    "\n",
    "wordform_df = pd.concat((df_GB1995[\"word\"],\n",
    "                         df_GB1995[\"plural/past/attrib\"],\n",
    "                         df_GB1995[\"diminu/compara/past plural\"],\n",
    "                         df_GB1995[\"past perfect/superla\"]))\\\n",
    "                .dropna()\n",
    "has_comma = wordform_df.str.contains(', ')\n",
    "wordform_df = pd.concat((wordform_df[~has_comma],) + tuple(pd.Series(row.split(', ')) for row in wordform_df[has_comma]))\n",
    "\n",
    "wordform_clean_df = clean_wordform_df(wordform_df)\n",
    "\n",
    "for marker, umarker in diacritic_markers.items():\n",
    "    wordform_clean_df = wordform_clean_df.str.replace(marker, umarker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if '\"' in w], [w for w in wordform_clean_df if ';' in w], [w for w in wordform_clean_df if '/' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[w for w in wordform_clean_df if 'Belgi' in w]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[[w for w in wordform_clean_df if d in w][:3] for d in diacritic_markers.values()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
