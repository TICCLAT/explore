{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "logging.basicConfig(format=\"%(asctime)s [%(process)d] %(levelname)-8s \"\n",
    "                    \"%(name)s,%(lineno)s\\t%(message)s\")\n",
    "logging.getLogger().setLevel('INFO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('../ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ticclat.dbutils import create_ticclat_database\n",
    "\n",
    "create_ticclat_database(dbname=os.environ['dbname'], user=os.environ['user'], passwd=os.environ['password'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus, WordformLink, WordformLinkSource\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope\n",
    "\n",
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elex_words_file = '/home/jvdzwaan/data/ticclat/TICCLAT/elex/e-Lex-1.1.uniq.utf8.txt'\n",
    "elex_lemma_file = '/home/jvdzwaan/data/ticclat/TICCLAT/elex/e-Lex-1.1.lemma_wordform.utf8.nonumbers.tsv'\n",
    "\n",
    "l_wf_pairs = pd.read_csv(elex_lemma_file, sep='\\t', header=None)\n",
    "l_wf_pairs.columns = ['lemma', 'variant']\n",
    "print(l_wf_pairs.shape)\n",
    "l_wf_pairs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wfs = pd.DataFrame()\n",
    "wfs['wordform'] = l_wf_pairs['lemma'].append(l_wf_pairs['variant'], ignore_index=True)\n",
    "wfs = wfs.drop_duplicates(subset='wordform')\n",
    "print(wfs.shape)\n",
    "wfs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.utils import write_json_lines, read_json_lines \n",
    "from ticclat.dbutils import add_lexicon, get_word_frequency_df\n",
    "\n",
    "def add_linked_lexicon(sesion, lexicon_name, vocabulary, wfs, from_column, to_column, from_correct, to_correct, batch_size=10000):\n",
    "    # Make a dataframe containing all wordforms in the lexicon\n",
    "    wordforms = pd.DataFrame()\n",
    "    wordforms['wordform'] = wfs[from_column].append(wfs[to_column], ignore_index=True)\n",
    "    wordforms = wordforms.drop_duplicates(subset='wordform')\n",
    "    \n",
    "    # Create the lexicon (with all the wordforms)\n",
    "    lexicon = add_lexicon(session, lexicon_name, vocabulary, wordforms, num=batch_size)\n",
    "    \n",
    "    # Add wordform links and source wordform links\n",
    "    #for idx, row in wfs.iterrows():\n",
    "    #    wf_from = session.query(Wordform).filter(Wordform.wordform == row[from_column]).first()\n",
    "    #    wf_to = session.query(Wordform).filter(Wordform.wordform == row[to_column]).first()\n",
    "        \n",
    "    #    wf_from.link_with_metadata(wf_to, from_correct, to_correct, lexicon)\n",
    "\n",
    "    return lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import add_lexicon, get_word_frequency_df\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    df = get_word_frequency_df(session, add_ids=True)\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "wf_mapping = defaultdict(int)\n",
    "wf_mapping = df['wordform_id'].to_dict(wf_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.dbutils import get_wf_mapping\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    wf_mapping = get_wf_mapping(session, lexicon_id=1)\n",
    "    \n",
    "print(len(wf_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_wf_pairs.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time\n",
    "from ticclat.ticclat_schema import WordformLink\n",
    "from ticclat.utils import write_json_lines, get_temp_file, json_line\n",
    "from sqlalchemy import exists, select\n",
    "\n",
    "def write_wf_links_and_wf_link_sources_to_add(session, wf_mapping, df, wf_from_name, wf_to_name, \n",
    "                                              lexicon_id, wf_from_correct, wf_to_correct, \n",
    "                                              wf_links_to_add_file, wf_link_sources_to_add_file):\n",
    "    num_wf_links = 0\n",
    "    num_wf_link_sources = 0\n",
    "    with open(wf_links_to_add_file, 'w') as links, open(wf_link_sources_to_add_file, 'w') as sources:\n",
    "        wf_links = defaultdict(bool)\n",
    "        for idx, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "            #print(row)\n",
    "            wf_from = wf_mapping[row[wf_from_name]]\n",
    "            wf_to = wf_mapping[row[wf_to_name]]\n",
    "            if wf_from != wf_to and (wf_from, wf_to) not in wf_links:  # don't add links to self! and keep track of what was added, because duplicates may occur\n",
    "                s = select([WordformLink]).where(WordformLink.wordform_from==wf_from and WordformLink.wordform_to==wf_to)\n",
    "                r = session.execute(s).fetchone()\n",
    "                if r is None:\n",
    "                    # Both directions of the relationship need to be added.\n",
    "                    links.write(json_line({'wordform_from': wf_from, 'wordform_to': wf_to}))\n",
    "                    links.write(json_line({'wordform_from': wf_to, 'wordform_to': wf_from}))\n",
    "                    \n",
    "                    num_wf_links += 2\n",
    "                # the wordform link sources (in both directions) need to be written regardless of the existence of the wordform links.\n",
    "                sources.write(json_line({'wordform_from': wf_from, 'wordform_to': wf_to, 'lexicon_id': lexicon_id, \n",
    "                                         'wordform_from_correct': wf_from_correct, 'wordform_to_correct': wf_to_correct}))\n",
    "                sources.write(json_line({'wordform_from': wf_to, 'wordform_to': wf_from, 'lexicon_id': lexicon_id, \n",
    "                                         'wordform_from_correct': wf_to_correct, 'wordform_to_correct': wf_from_correct}))\n",
    "                num_wf_link_sources += 2\n",
    "                \n",
    "                wf_links[(wf_from, wf_to)] = True\n",
    "                wf_links[(wf_to, wf_from)] = True\n",
    "    \n",
    "    return num_wf_links, num_wf_link_sources\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    wf_links_to_add_file = get_temp_file()\n",
    "    wf_link_sources_to_add_file = get_temp_file()\n",
    "    print('wf_links', wf_links_to_add_file)\n",
    "    print('wf_link_sources', wf_link_sources_to_add_file)\n",
    "    \n",
    "    num_l, num_s = write_wf_links_and_wf_link_sources_to_add(session, wf_mapping, l_wf_pairs, \n",
    "                                                             'lemma', 'variant', 1, True, True, \n",
    "                                                             wf_links_to_add_file, wf_link_sources_to_add_file)\n",
    "print(num_l, 'wordform links to add')\n",
    "print(num_s, 'wordform link sources to add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w, w_id in wf_mapping.items():\n",
    "    if w_id == 110:\n",
    "        print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_wf_pairs.query('lemma == \"aai\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_wf_pairs.query('variant == \"aai\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "aai is zowel lemma als variant met aaien. Dit resulteert in het proberen toe te voegen van duplicaten. Dus we moeten bijhouden wat al geschreven is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from ticclat.utils import read_json_lines\n",
    "from ticclat.sacoreutils import sql_insert_batches\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    sql_insert_batches(session, WordformLink, read_json_lines(wf_links_to_add_file))\n",
    "    sql_insert_batches(session, WordformLinkSource, read_json_lines(wf_link_sources_to_add_file))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "with session_scope(Session) as session:\n",
    "    add_linked_lexicon(session, 'e-Lex-1.1.lemma_wordform.utf8.nonumbers', True, l_wf_pairs, 'lemma', 'variant', True, True)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
