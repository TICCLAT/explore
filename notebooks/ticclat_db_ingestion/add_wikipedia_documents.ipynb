{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read information to connect to the database and put it in environment variables\n",
    "import os\n",
    "with open('../ENVVARS.txt') as f:\n",
    "    for line in f:\n",
    "        parts = line.split('=')\n",
    "        if len(parts) == 2:\n",
    "            os.environ[parts[0]] = parts[1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'ticclat'\n",
    "os.environ['dbname'] = db_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ticclat.ticclat_schema import Lexicon, Wordform, Anahash, Document, Corpus\n",
    "\n",
    "from ticclat.dbutils import get_session, session_scope\n",
    "\n",
    "Session = get_session(os.environ['user'], os.environ['password'], os.environ['dbname'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import textacy\n",
    "\n",
    "wiki = '/home/jvdzwaan/data/tmp/nlwiki-10000'\n",
    "\n",
    "c = textacy.Corpus(textacy.load_spacy('nl', disable=('parser', 'tagger')),\n",
    "               texts=textacy.io.read_text(wiki, lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "vectorizer = textacy.Vectorizer()\n",
    "doc_term_matrix = vectorizer.fit_transform(\n",
    "    (doc.to_terms_list(ngrams=1, named_entities=False, as_strings=True)for doc in c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This takes way too long. Also it crashes due to memory issues when we try to load the entire wikipedia.\n",
    "\n",
    "Okay, let's try some alternatives for loading the corpus and see how fast they are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load('nl', disable=['tagger', 'dep', 'ner', 'textcat'])\n",
    "\n",
    "i = 0\n",
    "\n",
    "wiki = '/home/jvdzwaan/data/tmp/nlwiki-1000'\n",
    "\n",
    "with open(wiki) as f:\n",
    "    for line in f:\n",
    "        doc = nlp(line.strip())\n",
    "        \n",
    "        i += 1\n",
    "        if i % 1000 == 0:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from collections import Counter\n",
    "import time\n",
    "from itertools import chain\n",
    "from nltk.corpus import brown\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "string_corpus = brown.raw()\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open(wiki) as f:\n",
    "    for line in f:\n",
    "        tokenized_corpus = [toktok.tokenize(sent) for sent in sent_tokenize(line.strip())]\n",
    "        fdist = Counter(chain(*tokenized_corpus))\n",
    "        \n",
    "        i += 1\n",
    "        #if i % 100000 == 0:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "import nltk.data\n",
    "\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/dutch.pickle')\n",
    "\n",
    "i = 0\n",
    "\n",
    "with open(wiki) as f:\n",
    "    for line in f:\n",
    "        tokenized_corpus = [word_tokenize(sent) for sent in tokenizer.tokenize(line.strip())]\n",
    "        fdist = Counter(chain(*tokenized_corpus))\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "        #if i % 1000 == 0:\n",
    "        #    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with session_scope(Session) as session:\n",
    "    print('number of wordforms:', session.query(Wordform).count())\n",
    "    print('number of lexica:', session.query(Lexicon).count())\n",
    "    print('number of documents:', session.query(Document).count())\n",
    "    print('number of corpora:', session.query(Corpus).count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Add all the wordforms that occur in the wikipedia dump\n",
    "from ticclat.tokenize import nltk_tokenize\n",
    "from ticclat.dbutils import bulk_add_wordforms\n",
    "\n",
    "num = 10000\n",
    "\n",
    "wiki = '/home/jvdzwaan/data/tmp/nlwiki'\n",
    "\n",
    "i = 0\n",
    "dfs = []\n",
    "with session_scope(Session) as session:\n",
    "    for terms_vector in tqdm(nltk_tokenize(wiki)):\n",
    "        df = pd.DataFrame()\n",
    "        df['wordform'] = terms_vector.keys()\n",
    "        #print(df.head())\n",
    "        dfs.append(df)\n",
    "        #print(terms_vector)\n",
    "        #print(pd.DataFrame.from_dict(terms_vector, orient='index'))\n",
    "        #break\n",
    "        i += 1\n",
    "    \n",
    "        if i % num == 0:\n",
    "            r = pd.concat(dfs)\n",
    "            r = r.drop_duplicates(subset='wordform')\n",
    "            n = bulk_add_wordforms(session, r, disable_pbar=True)\n",
    "            print('Added {} wordforms'.format(n))\n",
    "        \n",
    "            dfs = []\n",
    "\n",
    "    # also add the final documents\n",
    "    print(len(dfs))\n",
    "    if len(dfs) > 0:\n",
    "        r = pd.concat(dfs)\n",
    "        r = r.drop_duplicates(subset='wordform')\n",
    "        n = bulk_add_wordforms(session, r, disable_pbar=True)\n",
    "        print('Added {} wordforms'.format(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from ticclat.dbutils import add_corpus\n",
    "\n",
    "wiki = '/home/jvdzwaan/data/tmp/nlwiki-10'\n",
    "\n",
    "with session_scope(Session) as session:\n",
    "    c = add_corpus(session, 'nlwiki-20190201-pages-articles-23', wiki)\n",
    "    print('Added {} documents'.format(len(c.corpus_documents)))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
